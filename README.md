Данный проект является учебным. 
=========
Основной целью является практика использования **C++** и некоторых **Python-библиотек (numpy, polars, matplotlib и scikit-learn)**.

В рамках проекта был реализован класс **LinReg** и **LRSchedulerBase**.
Также для **LRSchedulerBase** были созданы классы-наследники: **StepDecay, ConstantLR, ExponentialDecay, CosineDecay**. Конечно же метод **fit()** класса **LinReg** 
может принимать в качестве параметра объект класса **LRSchedulerBase**.

Для проверки корректности реализации было проведено сравнение результатов обучения представителя класса **LinReg** и представителя класса **SGDRegressor** из **sklearn** (см. файл **./res.ipynb**).


Модель
==============

В классе **LinReg** реализован функционал для взаимодействия с линейной моделью:\
$f(x) = <w,x> + w_0, \hspace{0.5em} w,x \in \mathbb{R}^n,\hspace{0.2em} w_0 \in \mathbb{R}$.\
Обучение происходит посредством минимизации **MSE**. 
Также используется **L2-регуляризация**.\
В течение одной эпохи градиент вычисляется **по всем данным**. Это одно из отличий **LinReg** от **SGDRegressor**, в котором используется **SGD**, что видно из названия. Разделение данных на тренировочные и тестовые было произведено с соотношением **test_frac = 0.3**.

Данные
===============
Был использован датасет https://www.kaggle.com/datasets/camnugent/california-housing-prices. \
На этапе подготовки данных была выкинута колонка **oceanProximity** (для того чтобы не тратить время на кодирование признака, который изначально не является числовым).
В качестве таргета была выбрана колонка **medianHouseValue**.\
На этапе обработки происходит приведение значений признаков в диапазон $(-1, 1)$
посредством деления на максимум модуля (в случае если он не равен нулю).


Файлы проекта
==================

./LinearRegression/data/housing.csv
---------------
Оригинальный датасет.

./LinearRegression/data/orig_data.txt
---------------
Приведённый к нужному формату файл **housing.csv**.
Формат выглядит следующим образом:


n_obj dim\
X[0][0] X[0][1]  ...  X[0][dim - 1]  y[0]\
...\
X[n_obj - 1][0]  ...  X[n_obj - 1][dim - 1]  y[n_obj - 1]


Где X[ i ] [ j ] есть значение j-го признака у i-ой точки данных.


./LinearRegression/lr_exec.[cpp, py]
--------------------------
В файле **lr_exec.cpp** был реализован скрипт для запуска пайплайна 
"подготовка данных -> инициализация модели с заданными парметрами -> обучение -> рассчёт метрик" для представителя класса **LinReg**.

В файле **lr_exec.py** всё аналогично, но для представителя класса **SGDRegressor**.


./LinearRegression/main.py
--------------
При запуске файла **main.py** будет осуществлён перебор различных комбинаций гиперпараметров для cpp и py версий. Будет идти перебор комбинаций в цикле и на каждой итерации текущая комбинация будет подаваться в качестве параметров сразу для двух алгоритмов (т.е. для cpp и для py версии в итоге будет перебрано одно и то же мн-во комбинаций гиперпараметров). Результатом будут файлы **cpp_log.txt** и **py_log.txt**\
Интересующиеся могут обратить внимание на то, что в **main.py** есть функция для преобразования исходных данных посредством **PCA**, но она нигде не используется.
Это связано с тем, что эксперименты показали, что на рассматриваемом наборе данных понижение размерности не улучшает результата. А именно, не произойдёт уменьшение test_loss или train_loss (но скорость работы незначительно увеличится).\
 Это может быть связано с тем, что размерность данных и так достаточно мала (dim=8), и при этом каждый из признаков является информативным и признаки +- независимы.


./LinearRegression/logs/[cpp, py]_log.txt
---------------
Результат работы **main.py**. Каждая строка имеет формат\
__n_epochs initial_lr decay_rate regularization_alpha elapsed_time train_loss test_loss__


./res.ipynb
---------------
Здесь происходит сравнение **cpp_log.txt** и **py_log.txt**


Как запустить cpp и py версию и что произойдёт
=============
Cначала нужно скомпилировать cpp версию, например вот так:\
`g++ lr_exec.cpp LinReg.cpp -o lr_exec`

Для запуска cpp версии используется команда:\
`./lr_exec {n_epochs} {initial_lr} {decay_rate} {regularization_alpha}`

Аналогично для py версии:\
`python lr_exec.py {n_epochs} {initial_lr} {decay_rate} {regularization_alpha}`

В результате работы в стандартный вывод будет записано сообщение в cледующем формате:\
**elapsed_time train_loss test_loss**








