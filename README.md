Данный проект является учебным. 
=========
Основной целью является практика использования **C++** и некоторых **Python-библиотек (numpy, polars, matplotlib и scikit-learn)**.

В рамках проекта был реализован класс **LinReg** и **LRSchedulerBase**.
Также для **LRSchedulerBase** были созданы классы-наследники: **StepDecay, ConstantLR, ExponentialDecay, CosineDecay**. Конечно же метод **fit()** класса **LinReg** 
может принимать в качестве параметра объект класса **LRSchedulerBase**.

Для проверки корректности реализации было проведено сравнение результатов обучения представителя класса **LinReg** и представителя класса **SGDRegressor** из **sklearn**.


Модель
==============

В классе **LinReg** реализован функционал для взаимодействия с линейной моделью:\
$f(x) = <w,x> + w_0, \hspace{0.5em} w,x \in \mathbb{R}^n,\hspace{0.2em} w_0 \in \mathbb{R}$.\
Обучение происходит посредством минимизации **MSE**. 
Также используется **L2-регуляризация**.\
В течение одной эпохи градиент вычисляется **по всей выборке**. Это одно из отличий **LinReg** от **SGDRegressor**, в котором используется **SGD**, что видно из названия.

Данные
===============
Был использован датасет https://www.kaggle.com/datasets/camnugent/california-housing-prices.\
На этапе подготовки данных была выкинута колонка **oceanProximity** (для того чтобы не тратить время на кодирование признака, который изначально не является числовым).
В качестве таргета была выбрана колонка **medianHouseValue**.\
На этапе обработки происходит приведение значений признаков в диапазон $(-1, 1)$
посредством деления на максимум модуля (в случае если он не равен нулю).


Файлы проекта
==================

./LinearRegression/data/housing.csv
---------------
Оригинальный датасет.

./LinearRegression/data/orig_data.txt
---------------
Приведённый к нужному формату файл **housing.csv**.
Формат выглядит следующим образом:



$\text{n\_obj} \hspace{0.9em} \text{dim}\\
\begin{array}{c c c c c}
X_{0,0} & X_{0,1} & \cdots & X_{0,\text{dim} - 1} & y_0 \\
X_{1,0} & X_{1,1} & \cdots & X_{1,\text{dim} - 1} & y_1 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
X_{\text{n\_obj} - 1, 0} & X_{\text{n\_obj} - 1, 1} & \cdots & X_{\text{n\_obj} - 1, \text{dim} - 1} & y_{\text{n\_obj} - 1} \\
\end{array}\\$


Где $X_{i,j}$ есть значение j-го признака у i-ой точки данных.


./LinearRegression/lr_exec.[cpp, py]
--------------------------
В файле **lr_exec.cpp** был реализован скрипт для запуска пайплайна 
"подготовка данных -> инициализация модели с заданными парметрами -> обучение -> рассчёт метрик" для представителя класса LinReg.\
__*Далее будем считать, что имеется скомпилированный из lr_exec.cpp и LinReg.cpp\
 файл с названием lr_exec__


В файле **lr_exec.py** всё аналогично, но для представителя класса SGDRegressor.


./LinearRegression/main.py
--------------
При запуске файла **main.py** будет осуществлён перебор различных комбинаций гиперпараметров для cpp и py версий. Результатом будут файлы **cpp_log.txt** и **py_log.txt**\
Интересующиеся могут обратить внимание на то, что в **main.py** есть функция для преобразования исходных данных посредством **PCA**, но она нигде не используется.
Это связано с тем, что эксперименты показали, что на рассматриваемом наборе данных понижение размерности не улучшает результата. А именно, не произойдёт уменьшение test_loss или train_loss (но скорость работы незначительно увеличится).\
 Это может быть связано с тем, что размерность данных и так достаточно мала (dim=8), и при этом каждый из признаков является информативным и признаки +- независимы.


./LinearRegression/logs/[cpp, py]_log.txt
---------------
Результат работы **main.py**. Каждая строка имеет формат\
__n_epochs initial_lr decay_rate regularization_alpha elapsed_time train_loss test_loss__


./res.ipynb
---------------
Здесь происходит сравнение **cpp_log.txt** и **py_log.txt**


Как запустить cpp и py версию и что произойдёт
=============
Для запуска cpp версии используется команда:\
`./lr_exec {n_epochs} {initial_lr} {decay_rate} {regularization_alpha}`

Аналогично для py версии:\
`python lr_exec.py {n_epochs} {initial_lr} {decay_rate} {regularization_alpha}`

В результате работы в стандартный вывод будет записано сообщение в cледующем формате:\
__elapsed_time train_loss test_loss__








